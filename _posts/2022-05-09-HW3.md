---
layout: post
title: Dog or Not-Dog (Cat)
---

In this blog post, we will create several neural network models in order to distinguish between images of cats and dogs. We will work incrementally, implementing new techniques at each step to make improvements.

# Acknowledgment

Major parts of this Blog Post assignment, including several code chunks, are based on the [TensorFlow Transfer Learning](https://www.tensorflow.org/tutorials/images/transfer_learning) Tutorial. You may find that consulting this tutorial is helpful while completing this assignment, although this shouldn’t be necessary.

# §1. Load Packages and Obtain Data
To follow this tutorial, the following imports are required.


```python
import os
import tensorflow as tf
from tensorflow.keras import utils, layers, models
from matplotlib import pyplot as plt
```

Now, let’s access the data. We’ll use a sample data set provided by the TensorFlow team that contains labeled images of cats and dogs.


```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip
    68608000/68606236 [==============================] - 1s 0us/step
    68616192/68606236 [==============================] - 1s 0us/step
    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


This is technical code related to rapidly reading data. If you’re interested in learning more about this kind of thing, you can take a look [here](https://www.tensorflow.org/guide/data_performance).


```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

## Exploring the Data

Next, lets examine what the data looks like. We can write a function that visualizes 3 examples of both cats and dogs using Matplotlib.


```python
class_names = ['Cat', 'Dog']

def plot_class_examples(data):
    plt.figure(figsize=(9, 6))
    for images, labels in data.take(1):
        # Filter the images by label to ensure three of each will be plotted.
        cats = labels == 0
        lbls = tf.concat([labels[cats][:3], labels[~cats][:3]], 0)
        imgs = tf.concat([images[cats][:3], images[~cats][:3]], 0)
        for i in range(6):
            ax = plt.subplot(2, 3, i + 1)
            plt.imshow(imgs[i].numpy().astype("uint8"))
            plt.title(class_names[lbls[i]])
            plt.axis("off")

plot_class_examples(train_dataset)
```


    
![png](\images\hw3_10_0.png)
    


Let's also examine how many of each class exist in the training data.


```python
# Creates an iterator called labels
labels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()

labels = [label for label in labels_iterator]
print("Cats:", labels.count(0))
print("Dogs:", labels.count(1))
```

    Cats: 1000
    Dogs: 1000


We can observe that the baseline machine learning model will have an accuracy of 50% since equal amounts of cats and dogs exist in the training data.

# §2. First Model

Now, we can start by making our first basic model. By using the `summary()` method, we can view the number of parameters that will be trained in `model1`.


```python
model1 = models.Sequential([
    # Convolutional Layers
    layers.Conv2D(16, (3, 3), activation = 'relu', input_shape = (160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(16, (3, 3), activation = 'relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    
    # Fully Connected Layers
    layers.Dense(32, activation = 'relu'),
    layers.Dropout(0.2),
    layers.Dense(2, activation = 'softmax'),
])

model1.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d (Conv2D)             (None, 158, 158, 16)      448       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 79, 79, 16)       0         
     )                                                               
                                                                     
     conv2d_1 (Conv2D)           (None, 77, 77, 16)        2320      
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 38, 38, 16)       0         
     2D)                                                             
                                                                     
     flatten (Flatten)           (None, 23104)             0         
                                                                     
     dense (Dense)               (None, 32)                739360    
                                                                     
     dropout (Dropout)           (None, 32)                0         
                                                                     
     dense_1 (Dense)             (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 742,194
    Trainable params: 742,194
    Non-trainable params: 0
    _________________________________________________________________


We proceed by compiling `model1` using Adam as our optimizer, Sparse Categorical Crossentropy as our loss, and accuracy as our performance measurement.


```python
# Prepare model for training
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Train model
history1 = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 14s 57ms/step - loss: 25.8443 - accuracy: 0.5210 - val_loss: 0.7077 - val_accuracy: 0.4963
    Epoch 2/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6356 - accuracy: 0.6140 - val_loss: 0.7205 - val_accuracy: 0.5433
    Epoch 3/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.5865 - accuracy: 0.6860 - val_loss: 0.7197 - val_accuracy: 0.5532
    Epoch 4/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.4955 - accuracy: 0.7475 - val_loss: 0.7933 - val_accuracy: 0.5631
    Epoch 5/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.4251 - accuracy: 0.7760 - val_loss: 0.9501 - val_accuracy: 0.5631
    Epoch 6/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.3673 - accuracy: 0.8060 - val_loss: 0.9135 - val_accuracy: 0.5792
    Epoch 7/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.3119 - accuracy: 0.8440 - val_loss: 0.9080 - val_accuracy: 0.5718
    Epoch 8/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.2692 - accuracy: 0.8635 - val_loss: 1.0421 - val_accuracy: 0.5866
    Epoch 9/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.2358 - accuracy: 0.8815 - val_loss: 1.1577 - val_accuracy: 0.5569
    Epoch 10/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.2185 - accuracy: 0.8925 - val_loss: 1.2291 - val_accuracy: 0.5829
    Epoch 11/20
    63/63 [==============================] - 5s 71ms/step - loss: 0.2276 - accuracy: 0.8905 - val_loss: 1.2129 - val_accuracy: 0.5718
    Epoch 12/20
    63/63 [==============================] - 4s 53ms/step - loss: 0.2169 - accuracy: 0.8915 - val_loss: 1.2800 - val_accuracy: 0.5705
    Epoch 13/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1908 - accuracy: 0.9165 - val_loss: 1.4083 - val_accuracy: 0.5767
    Epoch 14/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.1754 - accuracy: 0.9195 - val_loss: 1.3822 - val_accuracy: 0.5743
    Epoch 15/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1628 - accuracy: 0.9285 - val_loss: 1.1495 - val_accuracy: 0.5804
    Epoch 16/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1521 - accuracy: 0.9210 - val_loss: 1.3060 - val_accuracy: 0.5978
    Epoch 17/20
    63/63 [==============================] - 4s 57ms/step - loss: 0.1604 - accuracy: 0.9215 - val_loss: 1.7135 - val_accuracy: 0.5817
    Epoch 18/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.1278 - accuracy: 0.9400 - val_loss: 1.4370 - val_accuracy: 0.5829
    Epoch 19/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.1298 - accuracy: 0.9375 - val_loss: 1.7421 - val_accuracy: 0.5829
    Epoch 20/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.1274 - accuracy: 0.9385 - val_loss: 1.3886 - val_accuracy: 0.5792


The validation accuracy of `model1` stabilized **around 58%** during training. This is a bit better than the baseline. Overfitting can be observed since the training accuracy is around 93%, which is significantly higher than the validation accuracy.

# §3. Model with Data Augmentation

The previous model suffers from overfitting. A solution to this problem could be implementing data augmentation. In this post, we will utilize `RandomFlip()` and `RandomRotation()` layers. First, we can write a function to examine what these layers do to an image.


```python
def plot_aug_examples(data, aug):
    plt.figure(figsize=(9, 6))
    for images, labels in data.take(1):
        # Take the first three images and apply the augmentation to them.
        imgs = aug(images[:3], training=True)
        lbls = tf.concat([labels[:3], labels[:3]], 0)
        imgs = tf.concat([images[:3], imgs], 0)
        for i in range(6):
            ax = plt.subplot(2, 3, i + 1)
            plt.imshow(imgs[i].numpy().astype("uint8"))
            plt.title(class_names[lbls[i]])
            plt.axis("off")
```


```python
# Plot RandomFlip() examples
plot_aug_examples(train_dataset, layers.RandomFlip('horizontal'))
```


    
![png](\images\hw3_21_0.png)
    



```python
# Plot RandomRotation() examples
plot_aug_examples(train_dataset, layers.RandomRotation(1))
```


    
![png](\images\hw3_22_0.png)
    


Now, let's implement these layers into a new model.


```python
model2 = models.Sequential([
    # Data Augmentation
    layers.RandomFlip('horizontal', input_shape = (160, 160, 3)),
    layers.RandomRotation(0.2),
    
    # Convolutional Layers
    layers.Conv2D(32, (3, 3), activation = 'relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation = 'relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    
    # Fully Connected Layers
    layers.Dense(64, activation = 'relu'),
    layers.Dropout(0.2),
    layers.Dense(2, activation = 'softmax'),
])

model2.summary()
```

    Model: "sequential_5"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     random_flip_24 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_6 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     conv2d_8 (Conv2D)           (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_8 (MaxPooling  (None, 79, 79, 32)       0         
     2D)                                                             
                                                                     
     conv2d_9 (Conv2D)           (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_9 (MaxPooling  (None, 38, 38, 32)       0         
     2D)                                                             
                                                                     
     flatten_4 (Flatten)         (None, 46208)             0         
                                                                     
     dense_9 (Dense)             (None, 64)                2957376   
                                                                     
     dropout_5 (Dropout)         (None, 64)                0         
                                                                     
     dense_10 (Dense)            (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 2,967,650
    Trainable params: 2,967,650
    Non-trainable params: 0
    _________________________________________________________________


We compile the model with the same arguments.


```python
# Prepare model for training
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Train model
history2 = model2.fit(train_dataset, 
                      epochs=20, 
                      validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 5s 58ms/step - loss: 41.0672 - accuracy: 0.5210 - val_loss: 0.7663 - val_accuracy: 0.5210
    Epoch 2/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.7125 - accuracy: 0.5355 - val_loss: 0.7009 - val_accuracy: 0.5260
    Epoch 3/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6870 - accuracy: 0.5560 - val_loss: 0.6924 - val_accuracy: 0.5520
    Epoch 4/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6867 - accuracy: 0.5550 - val_loss: 0.6927 - val_accuracy: 0.5384
    Epoch 5/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6853 - accuracy: 0.5615 - val_loss: 0.6907 - val_accuracy: 0.5606
    Epoch 6/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.6797 - accuracy: 0.5465 - val_loss: 0.6820 - val_accuracy: 0.5594
    Epoch 7/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6811 - accuracy: 0.5540 - val_loss: 0.6894 - val_accuracy: 0.5842
    Epoch 8/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.6866 - accuracy: 0.5560 - val_loss: 0.6947 - val_accuracy: 0.5545
    Epoch 9/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6748 - accuracy: 0.5750 - val_loss: 0.6873 - val_accuracy: 0.5644
    Epoch 10/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6794 - accuracy: 0.5725 - val_loss: 0.6928 - val_accuracy: 0.5334
    Epoch 11/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6651 - accuracy: 0.6010 - val_loss: 0.7028 - val_accuracy: 0.5507
    Epoch 12/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.6718 - accuracy: 0.5905 - val_loss: 0.6925 - val_accuracy: 0.5668
    Epoch 13/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6575 - accuracy: 0.6155 - val_loss: 0.7096 - val_accuracy: 0.5483
    Epoch 14/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6578 - accuracy: 0.6030 - val_loss: 0.7097 - val_accuracy: 0.5693
    Epoch 15/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6511 - accuracy: 0.6090 - val_loss: 0.7310 - val_accuracy: 0.5656
    Epoch 16/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.6629 - accuracy: 0.6075 - val_loss: 0.6905 - val_accuracy: 0.5718
    Epoch 17/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6516 - accuracy: 0.6340 - val_loss: 0.6987 - val_accuracy: 0.5569
    Epoch 18/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6552 - accuracy: 0.6210 - val_loss: 0.6869 - val_accuracy: 0.5693
    Epoch 19/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6427 - accuracy: 0.6080 - val_loss: 0.7232 - val_accuracy: 0.5804
    Epoch 20/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6468 - accuracy: 0.6190 - val_loss: 0.7041 - val_accuracy: 0.5755


The validation accuracy of `model2` stabilized **around 58%** during training. This is essentially the same as the accuracy from `model1`. Overfitting is not observed since the training accuracy is around 61%, which is similar to the validation accuracy.

# §4. Data Preprocessing

Adding data augmentation successfully reduced overfitting, however the accuracy of the model is still not very good. The original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. The following code will create a preprocessing layer called preprocessor which you can slot into your model pipeline.


```python
# Preprocess images
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, we add the `preprocessor` layer at the very beginning of our new model.


```python
model3 = models.Sequential([
    preprocessor,
    
    # Data Augmentation Layers
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    
    # Convolutional Layers
    layers.Conv2D(32, (3, 3), activation = 'relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(32, (3, 3), activation = 'relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dropout(0.2),
    
    # Output Layer
    layers.Dense(2, activation = 'softmax'),
])

model3.summary()
```

    Model: "sequential_6"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_25 (RandomFlip)  (None, 160, 160, 3)      0         
                                                                     
     random_rotation_7 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     conv2d_10 (Conv2D)          (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d_10 (MaxPoolin  (None, 79, 79, 32)       0         
     g2D)                                                            
                                                                     
     conv2d_11 (Conv2D)          (None, 77, 77, 32)        9248      
                                                                     
     max_pooling2d_11 (MaxPoolin  (None, 38, 38, 32)       0         
     g2D)                                                            
                                                                     
     flatten_5 (Flatten)         (None, 46208)             0         
                                                                     
     dropout_6 (Dropout)         (None, 46208)             0         
                                                                     
     dense_11 (Dense)            (None, 2)                 92418     
                                                                     
    =================================================================
    Total params: 102,562
    Trainable params: 102,562
    Non-trainable params: 0
    _________________________________________________________________


We compile `model3` with the same arguments as the previous models.


```python
# Prepare model for training
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Train model
history3 = model3.fit(train_dataset, 
                      epochs=20, 
                      validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6989 - accuracy: 0.5310 - val_loss: 0.6557 - val_accuracy: 0.5743
    Epoch 2/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.6450 - accuracy: 0.6030 - val_loss: 0.6277 - val_accuracy: 0.6361
    Epoch 3/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.6341 - accuracy: 0.6385 - val_loss: 0.6044 - val_accuracy: 0.6869
    Epoch 4/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.6134 - accuracy: 0.6545 - val_loss: 0.5977 - val_accuracy: 0.6733
    Epoch 5/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.5850 - accuracy: 0.6910 - val_loss: 0.6076 - val_accuracy: 0.7042
    Epoch 6/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.5791 - accuracy: 0.6900 - val_loss: 0.5807 - val_accuracy: 0.6993
    Epoch 7/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.5798 - accuracy: 0.6900 - val_loss: 0.5773 - val_accuracy: 0.6980
    Epoch 8/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.5568 - accuracy: 0.7025 - val_loss: 0.5840 - val_accuracy: 0.6968
    Epoch 9/20
    63/63 [==============================] - 4s 55ms/step - loss: 0.5613 - accuracy: 0.7095 - val_loss: 0.5802 - val_accuracy: 0.7079
    Epoch 10/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.5569 - accuracy: 0.7135 - val_loss: 0.5729 - val_accuracy: 0.6993
    Epoch 11/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.5446 - accuracy: 0.7105 - val_loss: 0.5513 - val_accuracy: 0.7302
    Epoch 12/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.5323 - accuracy: 0.7290 - val_loss: 0.5593 - val_accuracy: 0.7215
    Epoch 13/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.5492 - accuracy: 0.7185 - val_loss: 0.5320 - val_accuracy: 0.7339
    Epoch 14/20
    63/63 [==============================] - 4s 54ms/step - loss: 0.5315 - accuracy: 0.7350 - val_loss: 0.5284 - val_accuracy: 0.7512
    Epoch 15/20
    63/63 [==============================] - 4s 56ms/step - loss: 0.5313 - accuracy: 0.7345 - val_loss: 0.5367 - val_accuracy: 0.7240
    Epoch 16/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.5164 - accuracy: 0.7345 - val_loss: 0.5100 - val_accuracy: 0.7537
    Epoch 17/20
    63/63 [==============================] - 4s 58ms/step - loss: 0.5110 - accuracy: 0.7375 - val_loss: 0.5339 - val_accuracy: 0.7302
    Epoch 18/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.5252 - accuracy: 0.7360 - val_loss: 0.5019 - val_accuracy: 0.7636
    Epoch 19/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.5114 - accuracy: 0.7420 - val_loss: 0.5263 - val_accuracy: 0.7500
    Epoch 20/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.4992 - accuracy: 0.7525 - val_loss: 0.5406 - val_accuracy: 0.7327


The validation accuracy of `model3` stabilized **between 72% and 77%** during training. This is significantly better than the accuracy from `model2`. Overfitting is not observed since the training accuracy stabilized between 73% and 76%, which is similar to the validation accuracy.

# §5. Transfer Learning

The performance of `model3` is decent, but we can do better. So far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. We could use a pre-existing model for our task. To do this, we need to first access a pre-existing “base model”, incorporate it into a full model for our current task, and then train that model.

Use the following code in order to download MobileNetV2 and configure it as a layer that can be included in your model.


```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

# Prevent existing parameters from being updated during training
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step


To create `model4`, we remove our old convolutional layers and replace them with the MobileNetV2 layer.


```python
model4 = models.Sequential([
    preprocessor,
    
    # Data Augmentation Layers
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    
    # Transfer Learning Layer
    base_model_layer,
    layers.GlobalMaxPooling2D(),
    layers.Dropout(0.2),
    
    # Output Layer
    layers.Dense(2, activation = 'softmax'),
])

model4.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model (Functional)          (None, 160, 160, 3)       0         
                                                                     
     random_flip_2 (RandomFlip)  (None, 160, 160, 3)       0         
                                                                     
     random_rotation_2 (RandomRo  (None, 160, 160, 3)      0         
     tation)                                                         
                                                                     
     model_1 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d (Globa  (None, 1280)             0         
     lMaxPooling2D)                                                  
                                                                     
     dropout_3 (Dropout)         (None, 1280)              0         
                                                                     
     dense_6 (Dense)             (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________


Here, we note that while there are a lot of parameters, only 2,562 are trainable.

We compile `model4` the same way as the other models.


```python
# Prepare model for training
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Train model
history4 = model4.fit(train_dataset, 
                      epochs=20, 
                      validation_data=validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 8s 77ms/step - loss: 0.9390 - accuracy: 0.7870 - val_loss: 0.1313 - val_accuracy: 0.9641
    Epoch 2/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.3692 - accuracy: 0.9050 - val_loss: 0.0871 - val_accuracy: 0.9765
    Epoch 3/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2695 - accuracy: 0.9300 - val_loss: 0.1722 - val_accuracy: 0.9480
    Epoch 4/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.3128 - accuracy: 0.9180 - val_loss: 0.0683 - val_accuracy: 0.9827
    Epoch 5/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.3096 - accuracy: 0.9235 - val_loss: 0.1527 - val_accuracy: 0.9629
    Epoch 6/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2349 - accuracy: 0.9360 - val_loss: 0.0729 - val_accuracy: 0.9740
    Epoch 7/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2637 - accuracy: 0.9365 - val_loss: 0.0587 - val_accuracy: 0.9864
    Epoch 8/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.2351 - accuracy: 0.9345 - val_loss: 0.0568 - val_accuracy: 0.9802
    Epoch 9/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2282 - accuracy: 0.9395 - val_loss: 0.0585 - val_accuracy: 0.9839
    Epoch 10/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.1880 - accuracy: 0.9450 - val_loss: 0.0612 - val_accuracy: 0.9827
    Epoch 11/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.1961 - accuracy: 0.9500 - val_loss: 0.0815 - val_accuracy: 0.9777
    Epoch 12/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2057 - accuracy: 0.9430 - val_loss: 0.0942 - val_accuracy: 0.9728
    Epoch 13/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.2012 - accuracy: 0.9400 - val_loss: 0.0959 - val_accuracy: 0.9740
    Epoch 14/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.1931 - accuracy: 0.9475 - val_loss: 0.0763 - val_accuracy: 0.9839
    Epoch 15/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.2101 - accuracy: 0.9400 - val_loss: 0.0841 - val_accuracy: 0.9752
    Epoch 16/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.1882 - accuracy: 0.9510 - val_loss: 0.0989 - val_accuracy: 0.9691
    Epoch 17/20
    63/63 [==============================] - 4s 60ms/step - loss: 0.2192 - accuracy: 0.9390 - val_loss: 0.0568 - val_accuracy: 0.9802
    Epoch 18/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.1761 - accuracy: 0.9510 - val_loss: 0.0513 - val_accuracy: 0.9851
    Epoch 19/20
    63/63 [==============================] - 4s 61ms/step - loss: 0.2061 - accuracy: 0.9405 - val_loss: 0.0749 - val_accuracy: 0.9728
    Epoch 20/20
    63/63 [==============================] - 4s 59ms/step - loss: 0.1659 - accuracy: 0.9500 - val_loss: 0.0641 - val_accuracy: 0.9790


The validation accuracy of `model4` stabilized **around 98%** during training. This is significantly better than the accuracy from `model3`. Overfitting is not observed since the training accuracy stabilized around 95%, which is similar to the validation accuracy.

# §6. Score on Test Data

Based on all of the validation accuracies, `model4` is the best performer. Let's evaluate its performance on the unseen test data.


```python
# Evaluate model on test data
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 82ms/step - loss: 0.0505 - accuracy: 0.9844
    Test accuracy : 0.984375


Using `model4`, we get an accuracy of about 98% on the test data. This is great since it also matches the test and validation accuracies.
